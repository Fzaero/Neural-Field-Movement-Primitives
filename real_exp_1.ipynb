{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from datasets import Robot_Traj_Dataset_Experiment_Real_World_1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "\n",
    "from losses import task_loss_with_deform, task_loss\n",
    "from networks import NFSMP_Implicit, NFSMP_Implicit_Inference\n",
    "from visualize import img_predict, img_predict_inference, img_predict_interp, img_predict_interp_plot, contour_plot, contour_plot_interp\n",
    "from evaluate import validate_real_1\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "import trimesh\n",
    "import glob\n",
    "import open3d as o3d\n",
    "\n",
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_count = 18\n",
    "dataset = Robot_Traj_Dataset_Experiment_Real_World_1(traj_count)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1, num_workers=0, drop_last = True)\n",
    "fig,ax=plt.subplots(3,3,figsize=(18,18))\n",
    "for i in range(9):\n",
    "    ax[i//3,i%3].imshow(dataset.demo_img[i]/255.0)\n",
    "plt.show()\n",
    "# Shows trajectory samples\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "plt.figure(figsize=(16,16))\n",
    "mask = dataset.df[0]<1\n",
    "plt.scatter(dataset.xy[0,:,2][mask],dataset.xy[0,:,1][mask],c = dataset.df[0][mask],cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307404b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    total_steps=0\n",
    "    epochs=1200\n",
    "    lowest_mp_loss = 1000\n",
    "    for seed in range(10):\n",
    "        model = NFSMP_Implicit(traj_count//2)\n",
    "        model.to(device=torch.device('cuda:0'))\n",
    "        optim = torch.optim.Adam([\n",
    "                        {'params': model.mp_net.parameters()},\n",
    "                        {'params': model.sp_net.parameters()},\n",
    "                        {'params': model.deform_net_sp.parameters()},\n",
    "                        {'params': model.deform_net_mp.parameters()},\n",
    "                        {'params': model.hyper_net_mp_deform.parameters()},\n",
    "                        {'params': model.hyper_net_sp_deform.parameters()},\n",
    "                        {'params': model.latent_codes.parameters(),'lr':1e-4},\n",
    "                    ],\n",
    "            lr=1e-3)\n",
    "        with tqdm(total = len(dataloader) * epochs) as pbar:\n",
    "            train_losses = []\n",
    "            for epoch in range(epochs):\n",
    "                if epoch%200==199:\n",
    "                    img_predict_interp_plot(model,128,epoch,0,8)\n",
    "                    contour_plot_interp(model,epoch=epoch,index1=0,index2=8)\n",
    "                model.train()\n",
    "                for step, (model_input, gt) in enumerate(dataloader):\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    model_input = {key: value.cuda() for key, value in model_input.items()}\n",
    "                    gt = {key: value.cuda() for key, value in gt.items()}\n",
    "\n",
    "                    losses = model(model_input,gt,epoch)\n",
    "\n",
    "                    train_loss = 0.\n",
    "                    for loss_name, loss in losses.items():\n",
    "                        single_loss = loss.mean()\n",
    "\n",
    "                        if epoch %100== 0 and step==0:\n",
    "                            print(loss_name,single_loss.item()  )\n",
    "                        train_loss += single_loss            \n",
    "\n",
    "                    train_losses.append(train_loss.item())\n",
    "                    optim.zero_grad()\n",
    "                    train_loss.backward()\n",
    "                    optim.step()\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(loss=train_loss.item(), time=time.time() - start_time, epoch=epoch)\n",
    "                    total_steps += 1\n",
    "                if epoch>1000 and epoch%20==0:\n",
    "                    curr_error = validate_real_1(model)\n",
    "                    if curr_error<lowest_mp_loss:\n",
    "                        print(curr_error,epoch)\n",
    "                        lowest_mp_loss=curr_error\n",
    "                        checkpoint = copy.deepcopy(model.state_dict())\n",
    "else:\n",
    "    model = NFSMP_Implicit(traj_count//2)\n",
    "    model.to(device=torch.device('cuda:0'))\n",
    "    model.load_state_dict(torch.load(\"real_exp_1\"))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    torch.save(model.state_dict(), \"real_exp_1\")\n",
    "    print(lowest_mp_loss)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour_plot_interp(model,epoch=1000,index1=0,index2=6)\n",
    "# contour_plot_interp(model,epoch=1000,index1=0,index2=2)\n",
    "contour_plot_interp(model,epoch=1000,index1=8,index2=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3306a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_predict_interp_plot(model,256,1000,0,6)\n",
    "# img_predict_interp_plot(model,256,1000,0,2)\n",
    "img_predict_interp_plot(model,256,1000,0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import NFSMP_Implicit_Inference\n",
    "test_model = NFSMP_Implicit_Inference(9,model)\n",
    "test_model.to(device=torch.device('cuda:0'))\n",
    "optim = torch.optim.Adam([\n",
    "                {'params': test_model.latent_codes.parameters()},\n",
    "            ],\n",
    "    lr=0.1)\n",
    "val_dataloader = DataLoader(dataset, shuffle=False, batch_size=9, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32615709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "total_steps=0\n",
    "epochs=200\n",
    "with tqdm(total = len(val_dataloader) * epochs) as pbar:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%200==199:\n",
    "            img_predict(test_model,epoch=epoch*5)\n",
    "        test_model.train()\n",
    "        for step, (model_input, gt) in enumerate(val_dataloader):\n",
    "\n",
    "            start_time = time.time()\n",
    "            model_input = {key: value.cuda() for key, value in model_input.items()}\n",
    "            gt = {key: value.cuda() for key, value in gt.items()}\n",
    "            losses = test_model(model_input,gt,epoch*5)\n",
    "            train_loss = 0.\n",
    "            for loss_name, loss in losses.items():\n",
    "                single_loss = loss.mean()\n",
    "                if epoch %100== 0 and step==0:\n",
    "                    print(loss_name,single_loss)\n",
    "                train_loss += single_loss            \n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "            optim.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optim.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=train_loss, time=time.time() - start_time, epoch=epoch)\n",
    "            total_steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "fig,axs =plt.subplots(1,1,figsize=(12,4))\n",
    "t_condition = 0\n",
    "\n",
    "t_corr = int((t_condition+0.5)//(1/100))\n",
    "\n",
    "index1=2\n",
    "index2=0\n",
    "\n",
    "upper_mode = False\n",
    "lower_mode = True\n",
    "\n",
    "for p in range(6):\n",
    "    demo_x = np.linspace(-0.5,0.5,100).reshape(-1,1)\n",
    "    demo_y = np.zeros((100,3))\n",
    "    if upper_mode:\n",
    "        demo_y[:(t_corr+1)] = (dataset.y[0,t_corr,:]-dataset.y[0,0,:])/(t_corr+1)    \n",
    "#         demo_y[(t_corr+1):] = (dataset.y[0,99,:]-dataset.y[0,t_corr,:])/(t_corr+1)    \n",
    "    elif lower_mode:\n",
    "        demo_y[:(t_corr+1)] = (dataset.y[0,100+t_corr,:]-dataset.y[0,100,:])/(t_corr+1)\n",
    "#         demo_y[(t_corr+1):] = (dataset.y[0,199,:]-dataset.y[0,100+t_corr,:])/(t_corr+1)    \n",
    "    x_ = torch.from_numpy(demo_x).cuda().float().reshape(-1,1)\n",
    "    x_.requires_grad=False\n",
    "    y_dot = torch.from_numpy(demo_y).cuda().float()\n",
    "    y_dot.requires_grad=True\n",
    "    \n",
    "    optim = torch.optim.Adam([y_dot,], lr=3e-4)\n",
    "    \n",
    "    for i in range(50):\n",
    "        y_ = torch.zeros(100,3).cuda()\n",
    "        y_+= torch.from_numpy(dataset.y[0,0,:]).float().cuda()\n",
    "        for t in range(99):\n",
    "            y_[t+1] = y_[t] + y_dot[t]\n",
    "        samples = torch.cat([x_,y_],dim=1)\n",
    "\n",
    "        subject_idx = torch.Tensor([index1]).squeeze().long().cuda()[None,...]\n",
    "        subject_idx2 = torch.Tensor([index2]).squeeze().long().cuda()[None,...]\n",
    "        embedding = model.latent_codes(subject_idx)*(1-p/5.0)+model.latent_codes(subject_idx2)*p/5.0 \n",
    "        model_in = {'coords': samples}\n",
    "        hypo_params_mp = model.hyper_net_mp_deform(embedding)\n",
    "        deform =model.deform_net_mp(model_in,1000, params=hypo_params_mp)['model_out']  \n",
    "        model_in = {'coords': samples+deform}\n",
    "        mp  = model.mp_net(model_in,1000)['model_out']\n",
    "\n",
    "        z = mp\n",
    "        \n",
    "#         y_smoothness = 3e3*torch.mean((y_dot[1:-3]-y_dot[:-4])**2)\n",
    "        z_constraint = torch.mean(torch.abs(z[:i*4]))  \n",
    "        loss = z_constraint\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    xy = samples.detach().cpu().numpy()\n",
    "\n",
    "    axs.plot(xy[:99,2],xy[:99,1],color = cm(p*25),lw=3)    \n",
    "    \n",
    "if upper_mode:\n",
    "    axs.plot(dataset.y[index1,:100,1],dataset.y[index1,:100,0],color = 'darkred',lw=6,linestyle='--',label='Demonstration 0')\n",
    "    axs.plot(dataset.y[index2,:100,1],dataset.y[index2,:100,0],color = 'orange',lw=6,linestyle='--',label='Demonstration 2')\n",
    "elif lower_mode:\n",
    "    axs.plot(dataset.y[index1,100:200,1],dataset.y[index1,100:200,0],color = 'darkred',lw=6,linestyle='--',label='Demonstration 0')\n",
    "    axs.plot(dataset.y[index2,100:200,1],dataset.y[index2,100:200,0],color = 'orange',lw=6,linestyle='--',label='Demonstration 2')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim([0.35,-0.05])\n",
    "rect = patches.Rectangle((-0.20, 0.10), 0.20, 0.10, linewidth=1, edgecolor='black', facecolor='gray')\n",
    "axs.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "# axs.plot(dataset.y[index2,100:200,1],dataset.y[index2,100:200,0],color = 'orange',lw=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a48a8f",
   "metadata": {},
   "source": [
    "# Different Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c127fa",
   "metadata": {},
   "source": [
    "## Interpolation Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from functools import partial\n",
    "import matplotlib.patches as patches\n",
    "writer = animation.FFMpegWriter(\n",
    "    fps=15, metadata=dict(artist='Me'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RGB Interpolation Video Creation\n",
    "xlist = np.linspace(-0.5, 0.5, 1280)\n",
    "ylist = np.linspace(-0.5, 0.5, 720)\n",
    "X, Y = np.meshgrid(xlist, ylist)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot([], [], 'ro')\n",
    "Z_rgb = list()\n",
    "\n",
    "index_pairs = [(0,2),\n",
    "               (2,8),\n",
    "               (8,6)]\n",
    "for pair in index_pairs:\n",
    "    for index in range(30):\n",
    "        with torch.no_grad():\n",
    "            samples = torch.from_numpy(np.vstack([Y.reshape(-1),X.reshape(-1)])).float().permute(1,0).cuda()            \n",
    "            samples.requires_grad=False\n",
    "\n",
    "            subject_idx = torch.Tensor([pair[0]]).squeeze().long().cuda()[None,...]\n",
    "            embedding1 = model.latent_codes(subject_idx)    \n",
    "            subject_idx = torch.Tensor([pair[1]]).squeeze().long().cuda()[None,...]\n",
    "            embedding2 = model.latent_codes(subject_idx)    \n",
    "            embedding = embedding1*(1-index/29.0)+embedding2*(index/29.0)\n",
    "            out = model.inference(samples,embedding,epoch=1000)['rgb'].squeeze().detach().cpu().numpy()+0.5\n",
    "            out[out>1]=1\n",
    "            out[out<0]=0\n",
    "            Z_rgb.append(out.reshape(720,1280,3))\n",
    "fig,ax=plt.subplots(1,1,figsize=(64,36))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False) \n",
    "ims = []\n",
    "xlist = np.linspace(-0.3, 0.55, 32)\n",
    "ylist = np.linspace(-0.5, 0.35, 32)\n",
    "for i in range(len(Z_rgb)):\n",
    "    im = ax.imshow(Z_rgb[i], animated=True)\n",
    "    ims.append([im])\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "ani.save(\"exp3_rgb.mp4\", writer=writer)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb158113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cost Function Interpolation Video Creation\n",
    "\n",
    "xlist = np.linspace(-0.3, 0.55, 32)\n",
    "ylist = np.linspace(-0.5, 0.35, 32)\n",
    "zlist = np.linspace(0, 0.4, 32)\n",
    "tlist = np.linspace(-0.5, 0.5, 32)      \n",
    "X,T,   Y, Z = np.meshgrid(xlist,tlist,ylist,zlist)\n",
    "Z_mp = list() #y.reshape(64,64)\n",
    "\n",
    "index_pairs = [(0,2),\n",
    "               (2,8),\n",
    "               (8,6)]\n",
    "for pair in index_pairs:\n",
    "    for index in range(30):\n",
    "        with torch.no_grad():\n",
    "            samples = torch.from_numpy(np.vstack([Y.reshape(-1),X.reshape(-1)])).float().permute(1,0).cuda()            \n",
    "            samples.requires_grad=False\n",
    "\n",
    "            subject_idx = torch.Tensor([pair[0]]).squeeze().long().cuda()[None,...]\n",
    "            embedding1 = model.latent_codes(subject_idx)    \n",
    "            subject_idx = torch.Tensor([pair[1]]).squeeze().long().cuda()[None,...]\n",
    "            embedding2 = model.latent_codes(subject_idx)    \n",
    "            embedding = embedding1*(1-index/29.0)+embedding2*(index/29.0)\n",
    "            samples_mp = torch.from_numpy(np.vstack([T.reshape(-1),X.reshape(-1),Y.reshape(-1),Z.reshape(-1)])).float().permute(1,0).cuda()\n",
    "            samples_mp.requires_grad=False\n",
    "            out = model.inference_mp(samples_mp,embedding,epoch=1000)  \n",
    "            z_mp = out['mp'].squeeze().detach().cpu().numpy() \n",
    "            z_mp[z_mp>0.9]=0.9\n",
    "            z_mp[z_mp<0]=0\n",
    "            z_mp = z_mp.reshape(32,32,32,32)\n",
    "            Z_mp.append(z_mp)\n",
    "\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots(1,1,figsize=(64,36))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False) \n",
    "xlist = np.linspace(-0.3, 0.55, 32)\n",
    "ylist = np.linspace(-0.5, 0.35, 32)\n",
    "\n",
    "Y, X= np.meshgrid(ylist, xlist)\n",
    "\n",
    "# Method to change the contour data points\n",
    "def animate(index):\n",
    "    ax.clear()\n",
    "    ax.set_ylim([0.55,-0.3])\n",
    "    cp = ax.contourf(Y, X, Z_mp[index].min(axis=(0,3)))\n",
    "    if index<30:\n",
    "        rect_loc = 0.10\n",
    "        rect_size = 0.075\n",
    "        circle_loc = 0 + 0.30/29 * index\n",
    "    elif index<60:\n",
    "        rect_loc = 0.10 - 0.15 * (index-30)/29.0\n",
    "        rect_size = 0.075 + 0.30 * (index-30)/29.0\n",
    "        circle_loc = 0.30\n",
    "    else:\n",
    "        rect_loc = -0.05\n",
    "        rect_size = 0.375\n",
    "        circle_loc = 0.30 - 0.30/29 * (index-60)        \n",
    "    rect = patches.Rectangle((-0.18, rect_loc), 0.16, rect_size, linewidth=1, edgecolor='black',lw=20, facecolor='chocolate')\n",
    "    circle1 = plt.Circle((0.28, circle_loc), 0.06,color='black',fill=False,lw=100)\n",
    "    circle2 = plt.Circle((0.28, circle_loc), 0.06,color='gray',fill=False,lw=60)\n",
    "    ax.add_patch(rect)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    ax.set_xlabel(str(index))\n",
    "    if i == 0:\n",
    "        fig.colorbar(cp,ax=ax) # Add a colorbar to a plot\n",
    "\n",
    "# Call animate method\n",
    "ani = animation.FuncAnimation(fig, animate, len(Z_mp), interval=50, blit=False,repeat_delay=1000)\n",
    "ani.save(\"exp3_mp.mp4\", writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333f844",
   "metadata": {},
   "source": [
    "## Side-by-side Scene-motion View\n",
    "\n",
    "Need to create folder directory outputs/exp3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 9\n",
    "grid_size_ = 1.0 * (grid_size-1)\n",
    "\n",
    "corners ={\n",
    "    0 : (0,0),\n",
    "    2 : (0,grid_size-1),\n",
    "    6 : (grid_size-1,0),\n",
    "    8 : (grid_size-1,grid_size-1),\n",
    "}   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from functools import partial\n",
    "xlist = np.linspace(-0.5, 0.5, 256)\n",
    "ylist = np.linspace(-0.5, 0.5, 144)\n",
    "X, Y = np.meshgrid(xlist, ylist)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot([], [], 'ro')\n",
    "Z_rgb = list()\n",
    "\n",
    "corners =[0,2,6,8]\n",
    "\n",
    "def get_interp_embedding(x,y,embeddings):\n",
    "    R1 = embeddings[0]*(grid_size_-x)/grid_size_ + embeddings[1]*x/grid_size_\n",
    "    R2 = embeddings[2]*(grid_size_-x)/grid_size_ + embeddings[3]*x/grid_size_\n",
    "    P = R1 * (grid_size_-y)/grid_size_ + R2 * y/grid_size_\n",
    "    return P\n",
    "\n",
    "for axis1 in range(grid_size):\n",
    "    for axis2 in range(grid_size):\n",
    "        with torch.no_grad():\n",
    "            samples = torch.from_numpy(np.vstack([Y.reshape(-1),X.reshape(-1)])).float().permute(1,0).cuda()            \n",
    "            samples.requires_grad=False\n",
    "            embeddings = list()\n",
    "            weights = list()            \n",
    "            for corner in corners: \n",
    "                subject_idx = torch.Tensor([corner]).squeeze().long().cuda()[None,...]\n",
    "                embeddings.append(model.latent_codes(subject_idx))      \n",
    "\n",
    "            embedding = get_interp_embedding(axis1,axis2,embeddings)\n",
    "            out = model.inference(samples,embedding,epoch=1000)['rgb'].squeeze().detach().cpu().numpy()+0.5\n",
    "            out[out>1]=1\n",
    "            out[out<0]=0\n",
    "            Z_rgb.append(out.reshape(144,256,3))\n",
    "            \n",
    "xlist = np.linspace(-0.3, 0.55, 32)\n",
    "ylist = np.linspace(-0.5, 0.35, 32)\n",
    "zlist = np.linspace(0, 0.4, 32)\n",
    "tlist = np.linspace(-0.5, 0.5, 32)      \n",
    "X,T,   Y, Z = np.meshgrid(xlist,tlist,ylist,zlist)\n",
    "Z_mp = list()\n",
    "\n",
    "for axis1 in range(grid_size):\n",
    "    for axis2 in range(grid_size):\n",
    "        with torch.no_grad(): \n",
    "            embeddings = list()\n",
    "            weights = list()            \n",
    "            for corner in corners: \n",
    "                subject_idx = torch.Tensor([corner]).squeeze().long().cuda()[None,...]\n",
    "                embeddings.append(model.latent_codes(subject_idx))      \n",
    "\n",
    "            embedding = get_interp_embedding(axis1,axis2,embeddings)\n",
    "            \n",
    "            samples_mp = torch.from_numpy(np.vstack([T.reshape(-1),X.reshape(-1),Y.reshape(-1),Z.reshape(-1)])).float().permute(1,0).cuda()\n",
    "            samples_mp.requires_grad=False\n",
    "            out = model.inference_mp(samples_mp,embedding,epoch=1000)  \n",
    "            z_mp = out['mp'].squeeze().detach().cpu().numpy() \n",
    "            z_mp[z_mp>0.9]=0.9\n",
    "            z_mp[z_mp<0]=0\n",
    "            z_mp = z_mp.reshape(32,32,32,32)\n",
    "            Z_mp.append(z_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bilinear_interp_images\n",
    "# import os\n",
    "# os.mkdir(\"outputs\")\n",
    "# os.mkdir(\"outputs/exp3\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(64,36))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(64,36))\n",
    "xlist = np.linspace(-0.3, 0.55, 32)\n",
    "ylist = np.linspace(-0.5, 0.35, 32)\n",
    "\n",
    "Y, X= np.meshgrid(ylist, xlist)\n",
    "def draw(index):\n",
    "    ax.clear()    \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False) \n",
    "    ax.imshow(Z_rgb[index]) \n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False) \n",
    "    ax2.set_ylim([0.55,-0.3])\n",
    "    wall_size=index%grid_size\n",
    "    bottle_location = index//grid_size\n",
    "    \n",
    "    cp = ax2.contourf(Y, X, Z_mp[index].min(axis=(0,3)),levels=[0,0.2,0.4,0.6,0.8,1])\n",
    "    circle_loc = 0 + 0.30/grid_size_ * bottle_location\n",
    "    rect_loc = 0.10 - 0.15 * wall_size/grid_size_\n",
    "    rect_size = 0.075 + 0.30 * wall_size/grid_size_\n",
    "\n",
    "    rect = patches.Rectangle((-0.18, rect_loc), 0.16, rect_size, linewidth=1, edgecolor='black',lw=20, facecolor='chocolate')\n",
    "    circle1 = plt.Circle((0.28, circle_loc), 0.06,color='black',fill=False,lw=100)\n",
    "    circle2 = plt.Circle((0.28, circle_loc), 0.06,color='gray',fill=False,lw=60)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.add_patch(circle1)\n",
    "    ax2.add_patch(circle2)\n",
    "    fig.savefig('outputs/exp3/rgb_'+str(index)+'.png',bbox_inches='tight')   # save the figure to file\n",
    "    fig2.savefig('outputs/exp3/mp_'+str(index)+'.png',bbox_inches='tight')   # save the figure to file\n",
    "for index in range(grid_size**2):\n",
    "    draw(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for index in range(grid_size**2):\n",
    "    # read the images\n",
    "    img1 = cv2.imread('outputs/exp3/rgb_'+str(index)+'.png')\n",
    "    img2 = cv2.imread('outputs/exp3/mp_'+str(index)+'.png')\n",
    "    im_h = cv2.hconcat([img1, img2])\n",
    "    cv2.imwrite('outputs/exp3/'+str(index)+'.png', im_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NF Torch 2",
   "language": "python",
   "name": "mf_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
